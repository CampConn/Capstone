\documentclass[10pt,journal,compsoc, draftclsnofoot,onecolumn]{IEEEtran}

\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{epstopdf}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{alltt}                                           
\usepackage{float}
\usepackage{color}
\usepackage{url}
\usepackage{balance}
\usepackage{enumitem}
\usepackage{parskip}
\usepackage{pstricks, pst-node}
\usepackage[margin=0.75in]{geometry}
\geometry{textheight=8.5in, textwidth=6in}

\newcommand{\cred}[1]{{\color{red}#1}}
\newcommand{\cblue}[1]{{\color{blue}#1}}
\usepackage{geometry}

\usepackage{hyperref}
\input{pygments.tex}

\begin{document}

\title{
Design Document\\
3D Object Pose Tracking for Robotics Grasping\\
CS463 Spring 2019
}
\author{Connor Campbell, Chase McWhirt and Jiawei Mo}

\maketitle

\begin{abstract}
Robotic vision is an advanced topic that requires proper planning and consideration before attempting. In this project, a computer will be taught to recognize a robotic arm in an image by first narrowing down the region of the image it is likely in, then scanning the area with a neural networks. Different solutions as well as technologies used for achieving them are discussed.
\end{abstract}

\IEEEdisplaynontitleabstractindextext
\IEEEpeerreviewmaketitle

\pagebreak
\tableofcontents
\pagebreak
\section{Table of Changes}
\begin{center}
 \begin{tabular}{|p{0.3\linewidth}|p{0.3\linewidth}|p{0.3\linewidth}|}
  \hline
 Section & Original & New \\ [0.5ex]
 \hline\hline

Summary
&
Groups based on data for arm pose.
Strict description of neural network structure.
&
Groups roughly based on pose base on time stamps for images.
Neural network structure can be experimented with.
\\ \hline

Glossary
&
Mentions of Gimp and Caffe
&
Gimp and Caffe were removed, as we used Photoshop and FANN instead.
Definition for Support Vector Machine provided
\\ \hline

Glossary
&
Added support vector machines and matplotlib.
&
Added a barebones definition of support vector machines.
Also described matplotlib, the Python alternative to OpenCV.
\\ \hline

Stakeholders
&
Our teacher's assistant was Wesley Alexis
&
Our teacher's assistant is now Omeed Habibelahian
\\ \hline

Design Concerns
&
No mention of desired platform.
&
The platform the project is done on does not matter to the client.
\\ \hline

Design Aspects - Support Vector Machine
&
(Nothing about support vector machines.)
&
A new section containing information about support vector machines, the library used, and notes about turning an image into a prediction mask.
\\ \hline

Design Aspects - Neural Network API
&
We planned to use the Caffe framework
&
Used FANN Library instead
\\ \hline

Design Aspects - Qualifying Data
&
We planned to use Gimp.
&
We used Photoshop.
Data only collected from the areas of images immediately surrounding the arm.
\\ \hline

Design Aspects - Input Images
&
Running on Python with package called OpenCV.
&
OpenCV is used in Neural Network solution by coding with C++. Right now, Kmeans use numpy package to read images as matrices to process.
\\ \hline

\end{tabular}
\end{center}

\begin{center}
 \begin{tabular}{|p{0.3\linewidth}|p{0.3\linewidth}|p{0.3\linewidth}|}
  \hline
 Section & Original & New \\ [0.5ex]
 \hline\hline

Design Aspects - Kmeans Learning
&
N/A
&
Using sklearn package to build up learning model and prediction function. The packge is available on Python official website.
\\ \hline

Show Masks During Running - Kmeans
&
N/A
&
Using matplotlib packges to pop the generated masks to a window for testing. It is available from Python official site.
\\ \hline

Saving Generated Masks - Kmeans
&
N/A
&
Using misc from scipy package to save generated masks in .png format. It is available from Python official site.
\\ \hline



Conclusion
&
Ideal accuracy 80-90\%
&
Accuracy should be 90\% between the predictions masks only within the arm's bounds.
\\ \hline
 
 \end{tabular}
\end{center}
\pagebreak

\section{Introduction}
\subsection{Purpose}
The goal of this document is provide a clear path of execution to stakeholders about how this product development project will reach completion.
It should ensure that the client, the instructors, and the students all understand how this project will be completed.

\subsection{Scope}
This document will describe various technologies, design choices, and a measurable progress checklist that will guide the rest of this product development project.

\subsection{Context}
This document is being written not only as a road map for our stakeholder, but also for our teachers and teachers' assistants to follow.
The project must be feasible to complete to the specifications by March 22, the end of Winter term, 2019.
If the project isn't completed by this time, there is only minimal flexibility available before the Oregon State Engineering Expo on May 17.

\subsection{Summary}
The end goal of this project is to create a software that will accurately identify a robotic arm in a given image.
Several possible solution plans have been created with the help of the client.
The client is going to take many photos of the robotic arm.
A straw man solution will be generated by running a k-means clustering method on all of the images, which will yield a naive method for identifying parts of the robotic arm.
This will act as a base line: the other solutions will be considered successful if they prove to be better than the straw man solution.

The images will be categorized roughly based on the robotic arm's pose.
The images will be frames from a video of the arm in motion, and the groups will be based on the time stamps of each image.
The other solutions will work using those categorizations.

The first possible solution is to run k-means on each of the pose groups.
The other solution is to train small neural networks on each subgroup. The client suggested two hidden layers of eight nodes each for these networks, though the exact structure will be experimented with.
With three inputs and two outputs, it should not be too time consuming to train each neural network.
10\% of the data will be set aside to test the effectiveness of each neural network.

Finally, as a backup plan, if none of the solutions performs better than the straw man (or perhaps to run alongside the three solutions regardless) a neural network will be trained on all of the data (without pose nearest neighbor groupings) to see if it can get better results.
This will also be tested with 10\% of the data as well.

\section{Glossary}
\textbf{HSV}: Stands for Hue, Saturation and Value. It is a color model used for defining colors in computer graphics.

\textbf{K-means clustering}: A method of dividing a large set of data into smaller portions based on where the data points tend to cluster.

\textbf{Support Vector Machine}: A supervised learning model with classification and regression analysis.
For this project, data has binary classification (part of the arm or not part of the arm).
Thus, a linear regression analysis was chosen.

\textbf{Neural Network}: A type of machine learning algorithm that can be trained to solve a variety of problems. Several layers of nodes simulate the firing of neurons.

\textbf{OpenCV}: An open source library for computer vision field.
It was initially released in 2000 and written in C/C++.
It supports various operating system and has interfaces for different programming languages, including Python, C and C++. The latest version is 4.0.0.

\textbf{matplotlib.image}: A Python alternative to OpenCV.
Used purely to open images as a list of RGB values and save lists of RGB values as images (at least by the SVM implementation).

\textbf{RBG}: Stands for Red, Blue and Green. A different color model.

\section{Stakeholders and Design Concerns}
There are three primary stakeholders vested in this project.
The first is the client and sponsor, Cindy Grimm, whose primary concerns are completion of the project within particular specifications.
There are also the instructors, Kevin McGrath and Kirsten Winters, and the teaching assistant assigned to this project Omeed Habibelahian.
Their primary concerns are upholding mandatory standards of practice and quality assurance.
While in the context of the class, we should consider all stakeholders and concerns, this document will focus on the client's concerns.
The platform in which this project is done doesn't matter to the client.

\section{Design Aspects}
\subsection{Support Vector Machine}
The scikit-learn API contains a Support Vector Machine library.
The API provides different kernels that create different shaped class groupings.
A test will be done to find the best kernel for the problem (it is the linear kernel).
Each kernel performs a different version of a regression analysis in order to make predictions on new data.
Once the model is built (using supervised training), it can be fed RGB values to make a prediction.
Thus an image can be turned into a list using matplotlib.image and fed directly to the prediction function.
This will return a list of 0s and 1s (not part of the arm, part of the arm).
This list can be turned back into an image to create a ``prediction mask''.

\subsection{Neural Network API}
Our completed project will be using neural networks. Overall, we'll be using a supervised learning approach.
The Fast Artificial Neural Network (FANN) library will be used for developing this implementation \cite{3:online}.
It's free, open source, and relatively easy to use, with plenty of documentation.
The programs for generating and testing the neural networks will be kept flexible and easy to use, so that the client may make adjustments in the future if needed.

\subsection{Qualifying Data}
Supervised learning requires a neural network to be shown several example problems along with the desired outcome.
As such, not only will we need a large data set of images (which our client will be providing for us), but the images in our data will need to be properly labeled with the desired outcome, which needs to be done by hand.
Specifically, each pixel in the image will need to be labeled as either part of the arm or not part of the arm.
This will require the use of image editing software that can trace objects.
Our goal is to turn raw data into qualified, supervised data.
Specifically, the data will only come from the area around the arm. Our client already has a means of estimating the arm's location, so it is not necessary to draw data from an entire image for any particular image group.

Photoshop will be used to fulfill this need.
Photoshop is available through the Citrix receiver provided through the university.
This will increase accessibility to the software. In turn, we'll be able to quickly qualify data that our neural networks can train with.

\subsection{Input Images}
Once the sets of images of the arm are prepared, the next step is to read images into the software for training.
This will require storing images and information about them in a way that the software can understand, as well as the ability to manipulate that information.


In order to store and manipulate images, OpenCV will be used for the neural network implementation.
OpenCV is an open source C/C++ library aimed at solving computer vision problems \cite{1:online}.
OpenCV stores the pixels of an image in a matrix.
Depending on the type of picture, each pixel can hold various values representing their color.
Each value in the matrix can represent a variable of RGB (BGR instead of RGB in OpenCV functions) or gray scale.
A pixel's color is described and manipulated through the use of a channel, which stores many values related to the pixel and automatically adjusts related values when one is changed.
For instance, it can convert from BGR to HSV and vice versa, which allows for HSV to be used even if the image is based on the RGB color system.
For the purposes of this project, OpenCV will convert the colors of the pixels in an image into a set of values that the software will understand.

\subsection{Kmeans Learning}
In the latest version of the model, Kmeans is implemented by Python and its third party libraries. Packages, including numpy, sklearn, matplotlib and scipy, are available from Python official site \cite{4:online}. It reads image sets and store them in numpy array as matrix form. By dealing with those matrices, sklearn use modified input data, known as training data, to process Kmean fitting. This will generate the basic Kmean model. Then it uses this model to predict masks related to their original images. By analyzing the output masks with rectangle methods, it will sign white to pixels in the arm area. Relatively, pixels out of arm area will be sign black, which assists the model to generate masks with rational image pattern. 

\subsection{Mask Operation}
A method of narrowing down the general region where the arm can be found will be used to reduce the strain on the neural networks.
An object in an image will generally have similarly colored pixels.
The mask operation essentially looks for sudden changes in color within an image to determine approximate boundaries for different objects.
After obtaining an approximate area, the neural network will be tasked with providing a more precise identification.
The client suggests using the K-means algorithm for this as well.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.15\textwidth}
        \includegraphics[width=\textwidth]{unmasked.jpg}
        \caption{Before Masking}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.15\textwidth}
        \includegraphics[width=\textwidth]{soft_masking.jpg}
        \caption{Soft Masking}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.15\textwidth}
        \includegraphics[width=\textwidth]{hard_masking.jpg}
        \caption{Hard Masking}
    \end{subfigure}
    \caption{An example of a masking operation}\label{fig:animals}
\end{figure}

\noindent
Figure 1 demonstrates an example use of a masking operation \cite{2:online}.
In this project, the operation will be used to remove irrelevant information (i.e. anything that isn't the robotic hand). 

\section{Design Rationale}
Clearly, this project is beyond our expected level of knowledge.
With this in mind, it is clear that opting for tools that can be tried quickly and repeatedly is critical to a successful end product.
It is possible that we'll change our planned implementation along the way and will require the reworking of fundamental steps along our path.
With the tools that have been designated, sudden changes will not be catastrophic to the project.

\section{Conclusion}
Upon project completion, the computer will be able to identify the robot arm in real time.
At least one of the methods used will beat the straw man approach of k-mean clustering without applying nearest neighbor.
Ideally, the computer will be able to correctly identify about 90\% of the pixels shown as either part of the arm or not in real time.% with false positives being more acceptable than false negatives.
Specifically, the 90\% metric should apply to the region of an image with the arm in it, defined as the smallest rectangle in the image that includes the entirety of the arm.
However, if this arbitrary metric is not achieved, it will show that this approach will not generally be successful and an alternate approach (being developed by another person) will be used instead.
If this method proves successful before the deadline, there are also some stretch goals for the project, such as performing similar methods with objects for the robot to pick up.
Another stretch goal is to ensure that both methods can run simultaneously in real time.

\newpage
% bibliography
\nocite{*}%if nothing is referenced it will still show up in refs
\bibliographystyle{ieeetr}
\bibliography{refs}
%end bibliography
\end{document}